{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COVID.ipynb","provenance":[],"collapsed_sections":["afWXm72-2LwK","-prI7CAG2Sa_","ciOYzhv10o5i","KIy83iuM02L4"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"afWXm72-2LwK","colab_type":"text"},"source":["#Bibliothèques"]},{"cell_type":"markdown","metadata":{"id":"pVgeTeMbvV70","colab_type":"text"},"source":["\n","- On importe la bibliothèque NLTK qui qui nous aidera concernant le traitement de langage naturel.\n","- On importe from GENSIM.MODELS les deux models Word2Vec et FastText pour crée notre propre modele.\n","- On importe plusieurs bibliothéques qui nous aident à réaliser notre model.   "]},{"cell_type":"code","metadata":{"id":"zev5sU0BrOow","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"24eca8ee-0aec-43ae-b9b7-80ecbeb0fc7e"},"source":["from sklearn.model_selection import train_test_split\n","import collections\n","import numpy as np\n","import tensorflow as tf\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model,load_model\n","from keras.layers import *\n","from keras.layers.embeddings import Embedding\n","from keras.optimizers import Adam\n","from keras.losses import sparse_categorical_crossentropy\n","from keras import Sequential\n","from matplotlib import pyplot\n","from keras.utils.vis_utils import plot_model\n","import pydot\n","from keras import optimizers\n","from keras.layers import Flatten\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","from gensim.models import Word2Vec, keyedvectors,FastText\n","from keras_preprocessing.text import text_to_word_sequence\n","from bs4 import BeautifulSoup\n","import re"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-prI7CAG2Sa_","colab_type":"text"},"source":["# Les données"]},{"cell_type":"markdown","metadata":{"id":"D0TJSUctv9lQ","colab_type":"text"},"source":["La fonction du nettoyage de texte arabe"]},{"cell_type":"code","metadata":{"id":"WewL9JbHrcTr","colab_type":"code","colab":{}},"source":["def clean(text):\n","        # suppression des carac html\n","        example1 = BeautifulSoup(text, 'lxml')\n","\n","        # suppression des mentions\n","        example2 = re.sub(r'@[A-Za-z0-9]+', '', example1.get_text())\n","\n","        # suppression des liens\n","        ex3 = re.sub('https?://[A-Za-z0-9./]+', '', example2)\n","\n","        # suppression des hashtag\n","        final = re.sub(r'#[A-Za-z0-9]+', \"\", ex3)\n","        def remove_emojis(data):\n","            emoj = re.compile(\"[\"\n","                          u\"\\U0001f600-\\U0001F64F\"  # emoticons\n","                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                          u\"\\U00002500-\\U00002BEF\"  # chinese char\n","                          u\"\\U00002702-\\U000027B0\"\n","                          u\"\\U00002702-\\U000027B0\"\n","                          u\"\\U000024C2-\\U0001F251\"\n","                          u\"\\U0001f926-\\U0001f937\"\n","                          u\"\\U00010000-\\U0010ffff\"\n","                          u\"\\u2640-\\u2642\"\n","                          u\"\\u2600-\\u2B55\"\n","                          u\"\\u200d\"\n","                          u\"\\u23cf\"\n","                          u\"\\u23e9\"\n","                          u\"\\u231a\"\n","                          u\"\\ufe0f\"  # dingbats\n","                          u\"\\u3030\"\n","                          \"]+\", re.UNICODE)\n","            return re.sub(emoj, '', data)\n","        texte = remove_emojis(final)\n","        return texte"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vkh2c6axA8G","colab_type":"text"},"source":["puis on ouvre les fichiers text nécessaires et on lit chaque ligne du fichier ensuite on stocke ces lignes dans une liste, et on fait appel à les fonction de nettoyage"]},{"cell_type":"code","metadata":{"id":"QjvaHMxMsLM8","colab_type":"code","colab":{}},"source":["#Answers\n","file = open('COVID_Answers_CLASS.txt', 'r', encoding='utf-8')\n","text = file.read()\n","Ans = text.split(\"\\n\")\n","A_data = []\n","for A_da in Ans:\n","    A_data.append(clean(A_da))\n","#Question\n","file = open('COVID_Question.txt', 'r', encoding='utf-8')\n","text = file.read()\n","Ques = text.split(\"\\n\")\n","help=Ques[0].replace('\\ufeff','')\n","del Ques[0]\n","Ques.insert(0,help)\n","Q_data = []\n","for Q_da in Ques :\n","    Q_data.append(clean(Q_da))\n","file = open('help.txt', 'r', encoding='utf-8')\n","text = file.read()\n","Help = text.split(\"\\n\")\n","H_data = []\n","for H_da in Help :\n","    H_data.append(clean(H_da))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ciOYzhv10o5i","colab_type":"text"},"source":["# Prétraitement"]},{"cell_type":"markdown","metadata":{"id":"CbcQkscyxu46","colab_type":"text"},"source":["Lors de cette etape ,on divisé les phrases en mots ."]},{"cell_type":"code","metadata":{"id":"_JisHyvAt9wL","colab_type":"code","colab":{}},"source":["L2 = [text_to_word_sequence(q) for q in Q_data]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BkeIh_Xcx0Ty","colab_type":"text"},"source":["Dans cette etape on creée notre modele word2vect, et à l'aide de la fonctions *save_word2vec_format* on stocke notre modèle Word2Vec sous forme de fichier texte"]},{"cell_type":"code","metadata":{"id":"UEVWHEfcuBn3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"235adc3e-7e8b-4cc1-9416-9c181966ce7d"},"source":["ft_model2 = Word2Vec(sentences=L2, size=100, window=5, sample=1e-2, negative=10, sg=0, workers=10, min_count=1,iter=20)\n","ft_model2.wv.save_word2vec_format('COVID.txt', binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"nXH1V09X2Q82","colab_type":"text"},"source":["**Etape 1: TOKENIZE**\n","\n","\n","*   Son but c’est de symboliser les données, c'est à dire convertie le texte en valeurs numériques **; transformer chaque chaque mot en nombre ;** \n"," \n","\n","*   Cela permet au réseau neuronal d'effectuer des opérations sur les données d'entrée"]},{"cell_type":"code","metadata":{"id":"Ob5yGsUCuBzl","colab_type":"code","colab":{}},"source":["def tokenize(x):\n","    tokenizer = Tokenizer(char_level=False)\n","    tokenizer.fit_on_texts(x)\n","    return tokenizer.texts_to_sequences(x), tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nIQ8Saam2cXM","colab_type":"text"},"source":["*   Exemple :"]},{"cell_type":"code","metadata":{"id":"kPZRZdvT2lEu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"ae2ed054-b7e8-4232-e7b1-775f992ab0df"},"source":["text_sentences = ['ما هي أنواع الملابس المتوفرة للصيف','هل يوجد ملابس للصيف.']\n","text_tokenized, text_tokenizer = tokenize(text_sentences)\n","print(text_tokenizer.word_index)\n","print()\n","for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(sent))\n","    print('  Output: {}'.format(token_sent))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'للصيف': 1, 'ما': 2, 'هي': 3, 'أنواع': 4, 'الملابس': 5, 'المتوفرة': 6, 'هل': 7, 'يوجد': 8, 'ملابس': 9}\n","\n","Sequence 1 in x\n","  Input:  ما هي أنواع الملابس المتوفرة للصيف\n","  Output: [2, 3, 4, 5, 6, 1]\n","Sequence 2 in x\n","  Input:  هل يوجد ملابس للصيف.\n","  Output: [7, 8, 9, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_Vravxuy3Tqo","colab_type":"text"},"source":["**Etape 2: PAD**\n","\n","\n","*   Lorsque nous introduisons nos séquences d'ID de mot dans le modèle finale, les séquence doient avoir la même longueur.\n"," \n","\n","*   Pour ce faire, un remplissage est ajouté à toute séquence plus courte que la longueur maximale (c'est-à-dire plus courte que la phrase la plus longue)."]},{"cell_type":"code","metadata":{"id":"hVHIT1YjuB2R","colab_type":"code","colab":{}},"source":["def pad(x, length=None):\n","    if length is None:\n","        length = max([len(sentence) for sentence in x])\n","        print('Length Max: {}'.format(length))\n","        print()\n","    return pad_sequences(x, maxlen=length, padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MNS9HNUU3X_Z","colab_type":"text"},"source":["*   Exemple (Poursuivre l'exemple précédent)\n"]},{"cell_type":"code","metadata":{"id":"nmZVgYgh3cYr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"224123c7-efe3-45b3-8d04-f27d30bfdd6d"},"source":["test_pad = pad(text_tokenized)\n","\n","for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(np.array(token_sent)))\n","    print('  Output: {}'.format(pad_sent))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Length Max: 6\n","\n","Sequence 1 in x\n","  Input:  [2 3 4 5 6 1]\n","  Output: [2 3 4 5 6 1]\n","Sequence 2 in x\n","  Input:  [7 8 9 1]\n","  Output: [7 8 9 1 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NgFugMvl3rUd","colab_type":"text"},"source":["**Etape 3: Prétraitement**\n","\n","\n","*   La création d'un dictionnaire des `{mot:id}` pour les Questions et les réponses .\n","\n","*   Transfomer toutes les Questions et les Réponses à des vecteur.\n","*   Calcule de la taille du vocabulaire Q et A.\n","\n","*   la taille maximale des Q et A"]},{"cell_type":"code","metadata":{"id":"m19gETKNuB5K","colab_type":"code","colab":{}},"source":["def preprocess(Q):\n","    preprocess_Q, Q_tk = tokenize(Q)\n","    preprocess_Q = pad(preprocess_Q)\n","    return preprocess_Q, Q_tk"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WIi3HPLuwKQr","colab_type":"text"},"source":["Execution"]},{"cell_type":"code","metadata":{"id":"MxNMWPGgv6QB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"ecd92584-c9a4-41b1-de3d-2564c696d0f8"},"source":["preproc_Q, Q_tokenizer = preprocess(Q_data)\n","print(Q_tokenizer.word_index)\n","\n","max_Q_sequence_length = preproc_Q.shape[1]\n","Q_vocab_size = len(Q_tokenizer.word_index)+1\n","\n","print('Data Preprocessed')\n","print(\"Max Q sentence length:\", max_Q_sequence_length)\n","print(\"Q vocabulary size:\", Q_vocab_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Length Max: 29\n","\n","{'هل': 1, 'من': 2, 'كورونا': 3, 'كوفيد': 4, '19': 5, 'ما': 6, 'أن': 7, 'جائحة': 8, 'وباء': 9, 'أو': 10, 'هو': 11, 'فيروس': 12, 'في': 13, 'مرض': 14, 'يمكن': 15, 'علاجه': 16, 'هذا': 17, 'كيف': 18, 'مصدر': 19, 'الأمراض': 20, 'هي': 21, 'الفيروس': 22, 'لماذا': 23, 'عن': 24, 'يعتبر': 25, 'ينتقل': 26, 'كغيره': 27, 'كل': 28, 'التهويل': 29, 'والترهيب': 30, 'مثل': 31, 'بما': 32, 'التي': 33, 'إلى': 34, 'أي': 35, 'أدوية': 36, 'تنتقل': 37, 'حيواني': 38, 'الجديد': 39, 'فيروسي': 40, 'طريق': 41, 'البشر': 42, 'المضادات': 43, 'الحيوية': 44, 'فعّالة': 45, 'الوقاية': 46, 'توجد': 47, 'علاجات': 48, 'يمكنها': 49, 'الحماية': 50, 'الفيروسية': 51, 'الأخرى': 52, 'بمرض': 53, 'أصاب': 54, 'الطعام': 55, 'الإصابة': 56, 'على': 57, 'الإيدز': 58, 'مصاب': 59, 'عبر': 60, 'قبل': 61, 'أعراض': 62, 'بالمرض': 63, 'بفيروس': 64, 'هناك': 65, 'شخص': 66, 'علامات': 67, 'المرض': 68, 'كان': 69, 'الجسم': 70, 'لو': 71, 'الحساسية': 72, 'الأرجية': 73, 'تعد': 74, 'المزمنة': 75, 'يهدد': 76, 'صاحبها': 77, 'ينتشر': 78, 'السن': 79, 'تتفشى': 80, 'بسبب': 81, 'الفرق': 82, 'بين': 83, 'الإنفلونزا': 84, 'الموسمية': 85, 'سلوك': 86, 'داخل': 87, 'براز': 88, 'عدوى': 89, 'الممكن': 90, 'كبار': 91, 'وصغار': 92, 'بصفة': 93, 'أكبر': 94, 'اليوم': 95, 'مجيء': 96, 'لديكم': 97, 'علم': 98, 'بأن': 99, 'اسمه': 100, 'العادية': 101, 'ونزلات': 102, 'البرد': 103, 'وبين': 104, 'عدد': 105, 'وضع': 106, 'منطقة': 107, 'الشرق': 108, 'الأوسط': 109, 'ماذا': 110, 'نسخ': 111, 'مطورة': 112, 'يبقى': 113, 'حيا': 114, 'الملابس': 115, 'مميتا': 116, 'طفلا': 117, 'يعاني': 118, 'الربو': 119, 'كم': 120, 'الكورونا': 121, 'الصين': 122, 'لا': 123, 'العدوى': 124, 'العارضة': 125, 'اكتشفت': 126, 'الفاكهة': 127, 'والمكسرات': 128, 'والمثلجات': 129, 'والحلويات': 130, 'إذا': 131, 'نفسي': 132, 'فترة': 133, 'الوفيات': 134, 'ينبغي': 135, 'الأمر': 136, 'أفعل': 137, 'الشفاء': 138, 'بكورونا': 139, 'العلاج': 140, 'لكورونا': 141, 'يجب': 142, 'يصاب': 143, 'تستغرق': 144, 'حضانة': 145, 'لمرض': 146, 'أشعر': 147, 'أتلقى': 148, 'التطعيم': 149, 'مقلق': 150, 'بعد': 151, 'عرضة': 152, 'الشخص': 153, 'مدة': 154, 'كنت': 155, 'علاج': 156, 'المنزل': 157, 'العمل': 158, 'ضد': 159, 'الحليمي': 160, 'البشري': 161, 'التهاب': 162, 'السحايا': 163, 'الكزاز': 164, 'ولكن': 165, 'توقفت': 166, 'تظهر': 167, 'مباشرة': 168, 'به': 169, 'الأطفال': 170, 'أقل': 171, 'لفيروس': 172, 'مناعتهم': 173, 'قوية': 174, 'المساعدة': 175, 'التصدي': 176, 'منه': 177, 'أقوم': 178, 'مقاومة': 179, 'حصلت': 180, 'يعرف': 181, 'أنه': 182, 'مناعة': 183, 'العدد': 184, 'الحقيقي': 185, 'لإصابات': 186, 'العالم': 187, 'الأشخاص': 188, 'الذي': 189, 'تعافي': 190, 'المريض': 191, 'وفاته': 192, 'سبب': 193, 'تأخر': 194, 'إيجاد': 195, 'معدل': 196, 'وفيات': 197, 'اتخاذها': 198, 'مصابا': 199, 'بنزلة': 200, 'برد': 201, 'إصابتي': 202, 'ستضاعف': 203, 'مرضي': 204, 'يساعد': 205, 'المناعة': 206, 'الجيدة': 207, 'تمنع': 208, 'نسبة': 209, 'أعلى': 210, 'فصل': 211, 'الأمهات': 212, 'مواليدهن': 213, 'بكثافة': 214, 'المناطق': 215, 'الاستوائية': 216, 'الحارة': 217, 'العين': 218, 'أكثر': 219, 'لقاح': 220, 'دواء': 221, 'بالقلق': 222, 'أحمي': 223, 'متى': 224, 'الوقت': 225, 'بعض': 226, 'فاتني': 227, 'الكثير': 228, 'التحصيل': 229, 'الدراسي': 230, 'فما': 231, 'الورم': 232, 'هذه': 233, 'الجائحة': 234, 'أصيب': 235, 'أحد': 236, 'أفراد': 237, 'أسرتي': 238, 'بالأعراض': 239, 'الحادة': 240, 'الحالات': 241, 'ممارسة': 242, 'المراهقون': 243, 'أعرف': 244, 'أني': 245, 'خدمات': 246, 'التمنيع': 247, 'مقررا': 248, 'حيث': 249, 'طريقة': 250, 'الانتشار': 251, 'الاصابات': 252, 'تحافظ': 253, 'صحتك': 254, 'النفسية': 255, 'خلال': 256, 'الحجر': 257, 'الصحي': 258, 'عند': 259, 'الأدوية': 260, 'حال': 261, 'حدثت': 262, 'سمح': 263, 'الله': 264, 'يستطيع': 265, 'المدخن': 266, 'له': 267, 'والتعافي': 268, 'أساعد': 269, 'جسمي': 270, 'أصبت': 271, 'بحماية': 272, 'نفحص': 273, 'جهاز': 274, 'مناعتنا': 275, 'تتم': 276, 'لنتأكد': 277, 'جاهزيتنا': 278, 'للفيروس': 279, 'تقوية': 280, 'وما': 281, 'أهم': 282, 'الأغذية': 283, 'الرافعة': 284, 'للمناعة': 285, 'أبرز': 286, 'النصائح': 287, 'أجل': 288, 'نظام': 289, 'قوي': 290, 'يخضعون': 291, 'للعزل': 292, 'حاليا': 293, 'الملائم': 294, 'الاحتياطات': 295, 'الواجب': 296, 'الاحتياطيات': 297, 'للسفر': 298, 'المدة': 299, 'يعيشها': 300, 'خارج': 301, 'جسم': 302, 'الإنسان': 303, 'التلقيح': 304, 'المضاد': 305, 'للإنفلونزا': 306, 'معدٍ': 307, 'أثناء': 308, 'الحضانة': 309, 'ظهور': 310, 'أعراضه': 311, 'المكوث': 312, 'البيت': 313, 'الوسيلة': 314, 'الآمنة': 315, 'للوقاية': 316, 'مرة': 317, 'ثانية': 318, 'يشفى': 319, 'الحيوانات': 320, 'يؤثر': 321, 'المتقدمين': 322, 'هم': 323, 'المعرضون': 324, 'لخطر': 325, 'وخيم': 326, 'أمور': 327, 'أتجنبها': 328, 'المرء': 329, 'عديم': 330, 'الأعراض': 331, 'أضع': 332, 'كمامة': 333, 'لحماية': 334, 'الهواء': 335, 'أطلب': 336, 'الطبية': 337, 'للإصابة': 338, 'جديد': 339, 'الطرود': 340, 'القادمة': 341, 'ارتفاع': 342, 'حرارة': 343, 'الجو': 344, 'القضاء': 345, 'أتناول': 346, 'مشكلة': 347, 'صحية': 348, 'مزمنة': 349, 'أغيّر': 350, 'شيء': 351, 'ذلك': 352, 'بالملل': 353, 'البقاء': 354, 'أقضي': 355, 'المزيد': 356, 'اللعب': 357, 'الإنترنت': 358, 'والتواصل': 359, 'الاجتماعي': 360, 'والدراسة': 361, 'يستدعي': 362, 'القلق': 363, 'أصدقائي': 364, 'يلتزمون': 365, 'بقواعد': 366, 'التباعد': 367, 'الجسدي': 368, 'الخدمات': 369, 'أطيق': 370, 'الطريقة': 371, 'يلمسني': 372, 'بها': 373, 'نعيش': 374, 'فيه': 375, 'معا': 376, 'ستنتهي': 377, 'كي': 378, 'أتمكن': 379, 'العودة': 380, 'المدرسة': 381, 'وألتقي': 382, 'بأصدقائي': 383, 'مجددا': 384, 'منذ': 385, 'توقف': 386, 'والداي': 387, 'أصبحا': 388, 'يتجادلان': 389, 'كثيراً': 390, 'وفي': 391, 'رأيت': 392, 'أحدهما': 393, 'يسيء': 394, 'يؤذي': 395, 'الآخر': 396, 'إما': 397, 'لفظياً': 398, 'جسديًا': 399, 'بالأمان': 400, 'الآمن': 401, 'الجنس': 402, 'الراهن': 403, 'بإمكاني': 404, 'الرياضة': 405, 'بالفعل': 406, 'وتجعل': 407, 'صحتي': 408, 'حالة': 409, 'حرجة': 410, 'الإصابات': 411, 'يوجد': 412, 'الترهيب': 413, 'بوقاية': 414, 'حصيلة': 415, 'اخبار': 416, 'مستجدات': 417}\n","Data Preprocessed\n","Max Q sentence length: 29\n","Q vocabulary size: 418\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_WGH6hEn7VU3","colab_type":"text"},"source":["\n","\n","\n","\n","*   Création de vecteue one-hot des réponses\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"LQLGgKRYuB7w","colab_type":"code","colab":{}},"source":["Y=[]\n","for q in A_data:\n","    a=[0]*81\n","    a[int(q)]=1\n","    Y.append(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hGONKPO34IPH","colab_type":"text"},"source":["**Etape 4: Embbedings**\n","\n","\n","*   l’utilisation d'une matrice qui contient les représentations des mots en vecteurs réalisés avec plusieurs méthodes **(W2V )** ,donc ça sera nécessaire de remplacer chaque mot par un vecteur comme input du model.\n","\n","\n","*   Et Dans notre cas la taille de chaque vecteur est 100"]},{"cell_type":"code","metadata":{"id":"p2UQnm35uB-d","colab_type":"code","colab":{}},"source":["embeddings_index = {}\n","f = open('COVID.txt',encoding='utf-8')\n","\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","\n","f.close()\n","\n","\n","def embedding_matrix_creater(embedding_dimention):\n","  embedding_matrix = np.zeros((Q_vocab_size, embedding_dimention))\n","  for word, i in Q_tokenizer.word_index.items():\n","      embedding_vector = embeddings_index.get(word)\n","      if embedding_vector is not None:\n","          # words not found in embedding index will be all-zeros.\n","          embedding_matrix[i] = embedding_vector\n","  return embedding_matrix\n","\n","mat=embedding_matrix_creater(100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KIy83iuM02L4","colab_type":"text"},"source":["# Modèle"]},{"cell_type":"markdown","metadata":{"id":"78ZK_fJozaGX","colab_type":"text"},"source":["L'utilisation d'un modèle de classification, LSTM  et des couches dense avec une sortie  de 81 classe  qui ont des probabilités differentes on utlisant la fonction d'activation softmax.   "]},{"cell_type":"code","metadata":{"id":"Ri2QiTDiu5zn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"2e4dd708-45e3-4a3d-b886-2eabae0ab559"},"source":["input_length = max_Q_sequence_length\n","embedding_dim = 100\n","\n","Q_train,Q_test,A_train,A_test=train_test_split(preproc_Q,Y,test_size=0.05,random_state=10)\n","\n","model = Sequential()\n","model.add(Embedding(Q_vocab_size, embedding_dim,\n","            input_length = input_length,weights=[mat] , trainable = True))\n","model.add(Bidirectional(LSTM(128)))\n","model.add(Dense(64, activation = \"relu\"))\n","model.add(Dense(81, activation = \"relu\"))\n","model.add(BatchNormalization())\n","model.add(Dense(np.array(Y).shape[1], activation = \"softmax\"))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 29, 100)           41800     \n","_________________________________________________________________\n","bidirectional_4 (Bidirection (None, 256)               234496    \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 64)                16448     \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 81)                5265      \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 81)                324       \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 81)                6642      \n","=================================================================\n","Total params: 304,975\n","Trainable params: 304,813\n","Non-trainable params: 162\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S6RwNf8S09uU","colab_type":"text"},"source":["L'éxecution de l'entrainement necessite deux fonction. model.compile qui aide le modèle à corriger durant l'entrainement plusieurs paramétres, et model.fit qui lance l'entrainement avec les données d'entrées et d'autres paramétres ."]},{"cell_type":"code","metadata":{"id":"cU_tUPinxarf","colab_type":"code","colab":{}},"source":["rms = optimizers.RMSprop(lr=0.001)\n","model.compile(optimizer=rms, loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","history = model.fit(np.array(preproc_Q),np.array(Y), \n","          epochs=100,batch_size=32,\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdJzV3FTyPiM","colab_type":"text"},"source":["Evaluation du model "]},{"cell_type":"code","metadata":{"id":"YMeS572jxzG6","colab_type":"code","colab":{}},"source":["[a,b]=model.evaluate(np.array(Q_test),np.array(A_test))\n","print('loss : {} , accuracy: {}'.format(a,b))"],"execution_count":null,"outputs":[]}]}