{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SHOPPING.ipynb","provenance":[],"collapsed_sections":["xt-C-Nq-JpB8","v2SzTjK5JvP_","wUhNEuJFJ0uE","HSRzHlRbKBcU"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xt-C-Nq-JpB8","colab_type":"text"},"source":["#Bibliothèque"]},{"cell_type":"markdown","metadata":{"id":"pVgeTeMbvV70","colab_type":"text"},"source":["\n","- On importe la bibliothèque NLTK qui qui nous aidera concernant le traitement de langage naturel.\n","- On importe from GENSIM.MODELS les deux models Word2Vec et FastText pour crée notre propre modele.\n","- On importe plusieurs bibliothéques qui nous aident à réaliser notre model.   "]},{"cell_type":"code","metadata":{"id":"zev5sU0BrOow","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"6b9d3a95-9c03-47f7-fa22-a5881c2d31a8"},"source":["from sklearn.model_selection import train_test_split\n","import collections\n","import numpy as np\n","import tensorflow as tf\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model,load_model\n","from keras.layers import *\n","from keras.layers.embeddings import Embedding\n","from keras.optimizers import Adam\n","from keras.losses import sparse_categorical_crossentropy\n","from keras import Sequential\n","from matplotlib import pyplot\n","from keras.utils.vis_utils import plot_model\n","import pydot\n","from keras import optimizers\n","from keras.layers import Flatten\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","from gensim.models import Word2Vec, keyedvectors,FastText\n","from keras_preprocessing.text import text_to_word_sequence\n","from bs4 import BeautifulSoup\n","import re"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v2SzTjK5JvP_","colab_type":"text"},"source":["#Les données"]},{"cell_type":"markdown","metadata":{"id":"D0TJSUctv9lQ","colab_type":"text"},"source":["*Puis* en créer deux fonction de nettoyage de texte arabe"]},{"cell_type":"code","metadata":{"id":"WewL9JbHrcTr","colab_type":"code","colab":{}},"source":["def clean(text):\n","        # suppression des carac html\n","        example1 = BeautifulSoup(text, 'lxml')\n","\n","        # suppression des mentions\n","        example2 = re.sub(r'@[A-Za-z0-9]+', '', example1.get_text())\n","\n","        # suppression des liens\n","        ex3 = re.sub('https?://[A-Za-z0-9./]+', '', example2)\n","\n","        # suppression des hashtag\n","        final = re.sub(r'#[A-Za-z0-9]+', \"\", ex3)\n","        def remove_emojis(data):\n","            emoj = re.compile(\"[\"\n","                          u\"\\U0001f600-\\U0001F64F\"  # emoticons\n","                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                          u\"\\U00002500-\\U00002BEF\"  # chinese char\n","                          u\"\\U00002702-\\U000027B0\"\n","                          u\"\\U00002702-\\U000027B0\"\n","                          u\"\\U000024C2-\\U0001F251\"\n","                          u\"\\U0001f926-\\U0001f937\"\n","                          u\"\\U00010000-\\U0010ffff\"\n","                          u\"\\u2640-\\u2642\"\n","                          u\"\\u2600-\\u2B55\"\n","                          u\"\\u200d\"\n","                          u\"\\u23cf\"\n","                          u\"\\u23e9\"\n","                          u\"\\u231a\"\n","                          u\"\\ufe0f\"  # dingbats\n","                          u\"\\u3030\"\n","                          \"]+\", re.UNICODE)\n","            return re.sub(emoj, '', data)\n","        texte = remove_emojis(final)\n","        return texte"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vkh2c6axA8G","colab_type":"text"},"source":["puis on ouvre les fichiers text nécessaires et on lit chaque ligne du fichier ensuite on stocke ces lignes dans une liste, et on fait appel à les fonction de nettoyage"]},{"cell_type":"code","metadata":{"id":"QjvaHMxMsLM8","colab_type":"code","colab":{}},"source":["#Answers\n","file = open('Shopping_Answers.txt', 'rt', encoding='utf-8')\n","text = file.read()\n","Ans = text.split(\"\\n\")\n","Ans[0]=Ans[0].replace('\\ufeff','')\n","A_data = []\n","for A_da in Ans :\n","    A_data.append(clean(A_da))\n","#Question\n","file = open('Shopping_Question.txt', 'rt', encoding='utf-8')\n","text = file.read()\n","Ques = text.split(\"\\n\")\n","Ques[0]=Ques[0].replace('\\ufeff','')\n","Q_data = []\n","for Q_da in Ques :\n","    Q_data.append(clean(Q_da))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wUhNEuJFJ0uE","colab_type":"text"},"source":["#Prétraitement"]},{"cell_type":"markdown","metadata":{"id":"CbcQkscyxu46","colab_type":"text"},"source":["Lors de cette etape ,on divisé les phrases en mots ."]},{"cell_type":"code","metadata":{"id":"_JisHyvAt9wL","colab_type":"code","colab":{}},"source":["L2 = [text_to_word_sequence(q) for q in Q_data]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BkeIh_Xcx0Ty","colab_type":"text"},"source":["Dans cette etape on creée notre modele word2vect, et à l'aide de la fonctions *save_word2vec_format* on stocke notre modèle Word2Vec sous forme de fichier texte"]},{"cell_type":"code","metadata":{"id":"UEVWHEfcuBn3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"10563e60-3a21-40fb-c66d-744f653f4305"},"source":["ft_model2 = Word2Vec(sentences=L2, size=100, window=5, sample=1e-2, negative=10, sg=0, workers=10, min_count=1,iter=20)\n","ft_model2.wv.save_word2vec_format('SHOPPING.txt', binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"nXH1V09X2Q82","colab_type":"text"},"source":["**Etape 1: TOKENIZE**\n","\n","\n","*   Son but c’est de symboliser les données, c'est à dire convertie le texte en valeurs numériques **; transformer chaque chaque mot en nombre ;** \n"," \n","\n","*   Cela permet au réseau neuronal d'effectuer des opérations sur les données d'entrée"]},{"cell_type":"code","metadata":{"id":"Ob5yGsUCuBzl","colab_type":"code","colab":{}},"source":["def tokenize(x):\n","    tokenizer = Tokenizer(char_level=False)\n","    tokenizer.fit_on_texts(x)\n","    return tokenizer.texts_to_sequences(x), tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nIQ8Saam2cXM","colab_type":"text"},"source":["*   Exemple :"]},{"cell_type":"code","metadata":{"id":"kPZRZdvT2lEu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"eabd6dd0-38e5-49a5-9e3a-70913213c584"},"source":["text_sentences = ['ما هي أنواع الملابس المتوفرة للصيف','هل يوجد ملابس للصيف.']\n","text_tokenized, text_tokenizer = tokenize(text_sentences)\n","print(text_tokenizer.word_index)\n","print()\n","for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(sent))\n","    print('  Output: {}'.format(token_sent))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'للصيف': 1, 'ما': 2, 'هي': 3, 'أنواع': 4, 'الملابس': 5, 'المتوفرة': 6, 'هل': 7, 'يوجد': 8, 'ملابس': 9}\n","\n","Sequence 1 in x\n","  Input:  ما هي أنواع الملابس المتوفرة للصيف\n","  Output: [2, 3, 4, 5, 6, 1]\n","Sequence 2 in x\n","  Input:  هل يوجد ملابس للصيف.\n","  Output: [7, 8, 9, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_Vravxuy3Tqo","colab_type":"text"},"source":["**Etape 2: PAD**\n","\n","\n","*   Lorsque nous introduisons nos séquences d'ID de mot dans le modèle finale, les séquence doient avoir la même longueur.\n"," \n","\n","*   Pour ce faire, un remplissage est ajouté à toute séquence plus courte que la longueur maximale (c'est-à-dire plus courte que la phrase la plus longue)."]},{"cell_type":"code","metadata":{"id":"hVHIT1YjuB2R","colab_type":"code","colab":{}},"source":["def pad(x, length=None):\n","    if length is None:\n","        length = max([len(sentence) for sentence in x])\n","        print('Length Max: {}'.format(length))\n","        print()\n","    return pad_sequences(x, maxlen=length, padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MNS9HNUU3X_Z","colab_type":"text"},"source":["*   Exemple (Poursuivre l'exemple précédent)\n"]},{"cell_type":"code","metadata":{"id":"nmZVgYgh3cYr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"8c448c7d-d7b8-44ef-8669-f63e406b8fab"},"source":["test_pad = pad(text_tokenized)\n","\n","for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(np.array(token_sent)))\n","    print('  Output: {}'.format(pad_sent))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Length Max: 6\n","\n","Sequence 1 in x\n","  Input:  [2 3 4 5 6 1]\n","  Output: [2 3 4 5 6 1]\n","Sequence 2 in x\n","  Input:  [7 8 9 1]\n","  Output: [7 8 9 1 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NgFugMvl3rUd","colab_type":"text"},"source":["**Etape 3: Prétraitement**\n","\n","\n","*   La création d'un dictionnaire des `{mot:id}` pour les Questions et les réponses .\n","\n","*   Transfomer toutes les Questions et les Réponses à des vecteur.\n","*   Calcule de la taille du vocabulaire Q et A.\n","\n","*   la taille maximale des Q et A"]},{"cell_type":"code","metadata":{"id":"m19gETKNuB5K","colab_type":"code","colab":{}},"source":["def preprocess(Q,A):\n","    preprocess_Q, Q_tk = tokenize(Q)\n","    preprocess_A, A_tk = tokenize(A)\n","    preprocess_Q = pad(preprocess_Q)\n","    preprocess_A = pad(preprocess_A)\n","    preprocess_A = preprocess_A.reshape(*preprocess_A.shape, 1)\n","    return preprocess_Q, preprocess_A, Q_tk, A_tk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QI6GZUpw8Pdt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"b8177ebe-8570-4935-b1bf-1d452149313a"},"source":["preproc_Q, preproc_A, Q_tokenizer, A_tokenizer = preprocess(Q_data, A_data)\n","print(Q_tokenizer.word_index)\n","\n","print(preproc_Q[:1])\n","max_Q_sequence_length = preproc_Q.shape[1]\n","max_A_sequence_length = preproc_A.shape[1]\n","\n","Q_vocab_size = len(Q_tokenizer.word_index)+1\n","A_vocab_size = len(A_tokenizer.word_index)+1\n","\n","print('Data Preprocessed')\n","print(\"Max Q sentence length:\", max_Q_sequence_length)\n","print(\"Max A sentence length:\", max_A_sequence_length)\n","print(\"Q vocabulary size:\", Q_vocab_size)\n","print(\"A vocabulary size:\", A_vocab_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Length Max: 12\n","\n","Length Max: 23\n","\n","{'هل': 1, 'يوجد': 2, 'البلوزة': 3, 'اللون': 4, 'من': 5, 'النسائية': 6, 'منها': 7, 'القميص': 8, 'الرجالية': 9, 'منه': 10, 'الشيفون': 11, 'الجاكيت': 12, 'الرجالي': 13, 'ما': 14, 'الفستان': 15, 'المقاس': 16, 'الأزرق': 17, 'الرمادية': 18, 'الزرقاء': 19, 'النسائي': 20, 'الأسود': 21, 'قياس': 22, 'لارج': 23, 'البيضاء': 24, 'مقاس': 25, 'الأحمر': 26, 'الزهري': 27, 'السوداء': 28, 'هو': 29, 'الأبيض': 30, 'الحمراء': 31, 'الأصفر': 32, 'البنطال': 33, 'الخمري': 34, 'البنفسجي': 35, 'الفاتح': 36, 'التنورة': 37, 'نوع': 38, 'اكس': 39, 'يمكنني': 40, 'الرمادي': 41, 'أصغر': 42, 'أكبر': 43, 'الخمرية': 44, 'حجز': 45, 'ميديوم': 46, 'الخضراء': 47, 'الصفراء': 48, 'الجينز': 49, 'ألوان': 50, 'الوردي': 51, 'الصوف': 52, 'سعر': 53, 'أخرى': 54, 'لون': 55, 'الغامق': 56, 'الزهرية': 57, 'الساتان': 58, 'التفاحي': 59, 'الصيفية': 60, 'البرتقالي': 61, 'سمول': 62, 'الأخضر': 63, 'الزيتي': 64, 'الشتوية': 65, 'سعرها': 66, 'الموف': 67, 'القصيرة': 68, 'المتوفرة': 69, 'القياسات': 70, 'ان': 71, 'البنفسجية': 72, 'الوردية': 73, 'الغاء': 74, 'هي': 75, 'الكحلي': 76, 'و': 77, 'قياسها': 78, 'قماشة': 79, 'السماوية': 80, 'الأورانج': 81, 'الزيتية': 82, 'لم': 83, 'سعره': 84, 'بلوزة': 85, 'اخر': 86, 'السكري': 87, 'نوعها': 88, 'السكرية': 89, 'قماشتها': 90, 'الكمونية': 91, 'نسائية': 92, 'العنابية': 93, 'النيللي': 94, 'رجالية': 95, 'تبديل': 96, 'قياسه': 97, 'المموه': 98, 'نهائي': 99, 'تعجبني': 100, 'قميص': 101, 'الطويلة': 102, 'جاكيت': 103, 'القياس': 104, 'يعجبني': 105, 'نوعه': 106, 'قماشته': 107, 'الجلد': 108, 'جينز': 109, 'تبديلها': 110, 'نسائي': 111, '42': 112, '44': 113, '46': 114, '48': 115, 'الدهبي': 116, 'بنطال': 117, 'القطن': 118, 'كانت': 119, 'صغيرة': 120, '36': 121, 'رجالي': 122, 'المبطن': 123, 'المخمل': 124, 'قمصان': 125, 'رمادية': 126, 'الألوان': 127, 'شيفون': 128, 'صيفية': 129, 'البحري': 130, 'فستان': 131, 'صوف': 132, 'فاتح': 133, '38': 134, 'شتوية': 135, 'تبديله': 136, 'بلوز': 137, 'أزرق': 138, 'البلوز': 139, 'أسود': 140, 'ألوانها': 141, 'غامق': 142, 'كتان': 143, 'طويلة': 144, 'قطن': 145, '40': 146, 'البناطلين': 147, 'سوداء': 148, 'بيضاء': 149, 'حمراء': 150, 'بناطلين': 151, 'تنورة': 152, 'خمرية': 153, 'قصيرة': 154, 'بحري': 155, 'زيتية': 156, 'سماوية': 157, 'زهري': 158, 'أورانج': 159, 'ساتان': 160, 'الملابس': 161, 'زرقاء': 162, 'صفراء': 163, 'القمصان': 164, 'الجواكي': 165, 'أحمر': 166, 'لونه': 167, 'خضراء': 168, 'فساتين': 169, 'وردي': 170, 'أصفر': 171, 'أبيض': 172, 'الذي': 173, 'أنواع': 174, 'التي': 175, 'جلد': 176, 'بنفسجي': 177, 'جواكي': 178, 'زهرية': 179, 'خمري': 180, 'أنواعها': 181, 'الكتان': 182, 'الجوخ': 183, 'والرمادية': 184, 'مبطن': 185, 'الغامقة': 186, 'ورمادية': 187, 'السماوي': 188, 'حجزتها': 189, 'بنفسجية': 190, 'وردية': 191, 'مخمل': 192, 'نيللي': 193, 'سكري': 194, 'رمادي': 195, 'تنانير': 196, 'أريد': 197, 'ملابس': 198, 'للصيف': 199, 'جوخ': 200, 'اللبنفسجية': 201, 'قمت': 202, 'بحجزها': 203, 'رؤية': 204, 'تياب': 205, 'ثياب': 206, 'القطعة': 207, 'درجة': 208, 'للأزرق': 209, 'سكرية': 210, 'االبنفسجية': 211, 'كمونية': 212, 'عنابية': 213, 'معرفة': 214, 'قياسات': 215, 'صباح': 216, 'الخير': 217, 'صباحو': 218, 'مساالخير': 219, 'مساؤو': 220, 'مرحبا': 221, 'شكرا': 222, 'السلام': 223, 'عليكم': 224, 'ارجاع': 225, 'ارجاعها': 226, 'مانوعه': 227, 'مانوع': 228, 'االأسود': 229, 'حبري': 230, 'ماهو': 231, 'الرجاليةالحمراء': 232, 'االرجالية': 233, 'لنسائية': 234, 'التنانير': 235, 'الحمر': 236, 'الأبيص': 237, 'إلى': 238, 'الأن': 239, 'أستطيع': 240, 'اعرض': 241, 'لي': 242, 'اأسود': 243, 'وعها': 244, 'مقياس': 245, 'لوان': 246, 'ماشة': 247, 'لقياسات': 248, 'مألوان': 249}\n","[[216 217   0   0   0   0   0   0   0   0   0   0]]\n","Data Preprocessed\n","Max Q sentence length: 12\n","Max A sentence length: 23\n","Q vocabulary size: 250\n","A vocabulary size: 258\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hGONKPO34IPH","colab_type":"text"},"source":["**Etape 4: Embbedings**\n","\n","\n","*   l’utilisation d'une matrice qui contient les représentations des mots en vecteurs réalisés avec plusieurs méthodes **(W2V )** ,donc ça sera nécessaire de remplacer chaque mot par un vecteur comme input du model.\n","\n","\n","*   Et Dans notre cas la taille de chaque vecteur est 100"]},{"cell_type":"code","metadata":{"id":"p2UQnm35uB-d","colab_type":"code","colab":{}},"source":["embeddings_index = {}\n","f = open('SHOPPING.txt',encoding='utf-8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","\n","def embedding_matrix_creater(embedding_dimention):\n","  embedding_matrix = np.zeros((Q_vocab_size, embedding_dimention))\n","  for word, i in Q_tokenizer.word_index.items():\n","      embedding_vector = embeddings_index.get(word)\n","      if embedding_vector is not None:\n","          embedding_matrix[i] = embedding_vector\n","  return embedding_matrix\n","\n","mat=embedding_matrix_creater(100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSRzHlRbKBcU","colab_type":"text"},"source":["#Modèle"]},{"cell_type":"markdown","metadata":{"id":"mgl4UI1z6QHo","colab_type":"text"},"source":["\n","\n","*   Division de la base de données\n","*   L'utlisation de GRU et CNN \n","\n"]},{"cell_type":"code","metadata":{"id":"287CMb9DuCBG","colab_type":"code","colab":{}},"source":["input_length = max_A_sequence_length\n","output_length = max_Q_sequence_length\n","embedding_dim = 100\n","\n","tmp_x = pad(preproc_Q, max_A_sequence_length)\n","Q_train,Q_test,A_train,A_test=train_test_split(tmp_x,preproc_A,test_size=0.1,random_state=20)\n","\n","enc_in = Input(shape=(input_length,), dtype='float32', name='enc_input')\n","\n","embedding_layer = Embedding(input_dim=Q_vocab_size,\n","                            output_dim=embedding_dim,\n","                            input_length=input_length,\n","                            trainable=False,\n","                            weights=[mat],\n","                            name='enc_embedding')\n","\n","enc_embedded  =  embedding_layer(enc_in)\n","\n","encoded=GRU(units=256, return_sequences=True, name='en_GRU',activation='relu')(enc_embedded)\n","\n","encodedcnnf = Conv1D(128, 3, strides=1)(encoded)\n","encodedcnnfmaxpo = MaxPooling1D(pool_size=2,strides=2)(encodedcnnf)\n","\n","encodedcnnff = Conv1D(128, 3, strides=1)(encoded)\n","encodedcnnfmaxpoo = MaxPooling1D(pool_size=2, strides=2)(encodedcnnff)\n","\n","\n","encodedcnnfff = Conv1D(128, 3, strides=1)(encoded)\n","encodedcnnfmaxpooo = MaxPooling1D(pool_size=2, strides=2)(encodedcnnfff)\n","\n","\n","encodedcnn=Concatenate(axis=1)([encodedcnnfmaxpo, encodedcnnfmaxpoo, encodedcnnfmaxpooo])\n","\n","encodedcnnfff = Conv1D(256, 6, strides=1,activation='relu')(encodedcnn)\n","encodedcnnfmax = MaxPooling1D(pool_size=3, strides=1)(encodedcnnfff)\n","\n","fc2 = TimeDistributed(Dense(units=A_vocab_size+1), name='fc2')(encodedcnnfmax)\n","preds = Activation('softmax', name='softmax')(fc2)\n","\n","model = Model(enc_in, preds)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-z6-m-y69iV","colab_type":"text"},"source":["\n","\n","*   L'entrainement\n","\n"]},{"cell_type":"code","metadata":{"id":"_qRzgeln6gHb","colab_type":"code","colab":{}},"source":["rms = optimizers.RMSprop(lr=0.001)\n","model.compile(optimizer=rms, loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n","\n","history = model.fit(Q_train,A_train, \n","          epochs=60, batch_size=1024, \n","          validation_split = 0.1\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-XLDfHHx5zEN","colab_type":"text"},"source":["Le réseau neuronal traduira l'entrée en ID (mot), ce qui n'est pas la forme finale que nous voulons. La fonction logits_to_text fera le pont entre les ids du réseau neuronal et les réponses."]},{"cell_type":"code","metadata":{"id":"bsC_qhpS9hbF","colab_type":"code","colab":{}},"source":["def logits_to_text(logits, tokenizer):\n","    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n","    index_to_words[0] = '<PAD>'\n","    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Uv53TuzAsLg","colab_type":"text"},"source":["evaluation"]},{"cell_type":"code","metadata":{"id":"fiE0-F4Y-Kiz","colab_type":"code","colab":{}},"source":["[a,b]=model.evaluate(Q_test, A_test)\n","print('loss : {} , accuracy: {}'.format(a,b))"],"execution_count":null,"outputs":[]}]}